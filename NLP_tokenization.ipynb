{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Let's learn how to tokenize sentences (i.e., to encode them such that understandable for computer)"
      ],
      "metadata": {
        "id": "-XsMRI00EYTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer # Toenizer is a class"
      ],
      "metadata": {
        "id": "KiHEPTmZEWEc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "L7PqYr6MA9Co"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We start by creating an instance (an object) of the class \"Tokenizer\". \n",
        "# Note: num_words limits the number of tokenized words, in case we deal with very large texts such as a book.\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "\n",
        "# Then we tell our tokenizer object to go through all sentences and fit itself to them:\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# The full list of words becomes available as the tokenizer's \"word_index\" property.\n",
        "# word_index: It is a dictionary that is mapping words (str) to their rank/index (int). Only set after fit_on_texts is called.\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "ifC-sdDqC1TO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the tokenized words:"
      ],
      "metadata": {
        "id": "94IkPXVFPMhi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02gqlcXMDK6l",
        "outputId": "a2060fc6-81e2-4071-edbb-74115897bb44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And now we can create sequences of numbers for our sentences.\n",
        "# The Tokenizer class supports a method called \"text_to_sequences\" \n",
        "# which creates sequences of tokens representing each sentence:"
      ],
      "metadata": {
        "id": "QHoP4DhkPVcU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "metadata": {
        "id": "TyjFhB9UDRK7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the results:"
      ],
      "metadata": {
        "id": "YQjAHwstQUk6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6ebWUY4DYs4",
        "outputId": "8918b8f4-f921-4293-d307-1f2b388124bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# So our basic tokenization is done. But there is a catch!\n",
        "# These all work in the training. But what if in the testing\n",
        "# there are words that the NN model has never seen? Like \"really\"? This can confuse the Tokenizer:"
      ],
      "metadata": {
        "id": "fbIh8VcMQgWL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [\n",
        "    'i really love my dog',\n",
        "    'my dog loves my manatee'\n",
        "]"
      ],
      "metadata": {
        "id": "J5lG0ujIS7vb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5PwTlWuTExz",
        "outputId": "b09fee5f-88f0-4f49-9077-aa6b580656e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 2, 1, 3], [1, 3, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that for example the [1, 3, 1] vector does not include tokenized value for the word \"love\"\n",
        "# or the [4, 2, 1, 3] vector does not include tokenized value for words \"loves\" and \"manatee\"!\n",
        "# We have essentially the length of our sequences do not correspond to the length of the sentences.\n",
        "# We need a little trick here to maintain the sequence length.\n",
        "# we can use the follwings:"
      ],
      "metadata": {
        "id": "aBnCRYQBTY6I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This way, the tokenizer will create a token for the word \"<OOV>\", and then\n",
        "# replace all words that it does not recognize with the Out Of Vocabulary token instead.\n",
        "# So now let's re-write the code as follows:"
      ],
      "metadata": {
        "id": "sPeNFn1zU1O-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "\n",
        "]\n",
        "tokenizer_new = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
        "tokenizer_new.fit_on_texts(sentences)\n",
        "word_index = tokenizer_new.word_index"
      ],
      "metadata": {
        "id": "7CEsBh5BV8he"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This should the trick. Let's see the tokenized values:"
      ],
      "metadata": {
        "id": "OF3tXn_RWbD_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index_new = tokenizer_new.word_index"
      ],
      "metadata": {
        "id": "aAGKUhidUb1Q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jN_ef7pWwqZ",
        "outputId": "2f85a61f-dc95-451b-c636-044da16d5211"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As we see, the tokenizer has created a toket for \"<OOV>\"."
      ],
      "metadata": {
        "id": "GepFegTybwPY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we want to convince ourselves, that with this trick the tokenized test sentences have the correct number of elements, i.e., 5:"
      ],
      "metadata": {
        "id": "PoYtqhpwawQW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq = tokenizer_new.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnqigQQ2ayJM",
        "outputId": "9f6ef440-a14a-4fef-e724-302206d1dae1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# et voila! The tokenizer has maintained the sequence length to be the same length as the sentence\n",
        "# (Each of the test sentences has as manny elements as its corresponding sentence.)"
      ],
      "metadata": {
        "id": "vYTkhoFEZTmy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok now we wonder how a NN handles sentences of different length.The simple solution is \"padding\". With this tequnique all sequences will have the same length:"
      ],
      "metadata": {
        "id": "KbGK8zsIcIxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "KgSBtp6XcsmJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(sequences)"
      ],
      "metadata": {
        "id": "9oZIe91wdkXZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX97aADYdXC5",
        "outputId": "986dcdf1-0ee2-40e1-fc99-368898208d4d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCf6I2-zdZWP",
        "outputId": "294caca1-f3dd-41a7-d3d1-134cb37dfc71"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  4  2  1  3]\n",
            " [ 0  0  0  4  2  1  6]\n",
            " [ 0  0  0  5  2  1  3]\n",
            " [ 7  5  8  1  3  9 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so we now have the same length for all sequences (i.e., the lengthj of the longest sentence.)\n",
        "# The padding are set to the left. If we want them to the right we just set the padding parameter as follows:"
      ],
      "metadata": {
        "id": "uVo81_RidtvC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(sequences, padding='post')"
      ],
      "metadata": {
        "id": "-6JapuEyePOR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKyXgA9Jd1hK",
        "outputId": "3186aa08-140c-494e-d496-82e35780b732"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4  2  1  3  0  0  0]\n",
            " [ 4  2  1  6  0  0  0]\n",
            " [ 5  2  1  3  0  0  0]\n",
            " [ 7  5  8  1  3  9 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can even set a maximum length for the sequences:"
      ],
      "metadata": {
        "id": "WirzyuCNegjn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(sequences, padding='post',\n",
        "                       maxlen=5)"
      ],
      "metadata": {
        "id": "mRDI3uaYek_f"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7ucvDpad1js",
        "outputId": "de72f60d-e0bf-45fd-babe-af869c0e2cb6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4  2  1  3  0]\n",
            " [ 4  2  1  6  0]\n",
            " [ 5  2  1  3  0]\n",
            " [ 8  1  3  9 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As we see the two first tokenized values are taken out.\n",
        "# If we want, we can have the two last tokenized values to be taken out:"
      ],
      "metadata": {
        "id": "x1s_e3iNeyxf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(sequences, padding='post',\n",
        "                       maxlen=5,\n",
        "                       truncating='post')"
      ],
      "metadata": {
        "id": "MqRgajaEfCff"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the difference in the results\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPqfbM_rfCiZ",
        "outputId": "12b5ed7b-7a11-43d6-f76e-a70a7e32c889"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4 2 1 3 0]\n",
            " [4 2 1 6 0]\n",
            " [5 2 1 3 0]\n",
            " [7 5 8 1 3]]\n"
          ]
        }
      ]
    }
  ]
}
# Source: TensorFlow videos on YouTube, e.g., https://www.youtube.com/watch?v=r9QjkdSJZ2g&list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S&index=2 
